{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70689704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6dad92",
   "metadata": {},
   "source": [
    "# RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea12c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "\n",
    "    def __init__(self, n_visible, n_hidden, lr=0.001, epochs=5, mode='bernoulli', batch_size=32, k=3, optimizer='adam', gpu=False, savefile=None, early_stopping_patience=5):\n",
    "        self.mode = mode # bernoulli or gaussian RBM\n",
    "        self.n_hidden = n_hidden #  Number of hidden nodes\n",
    "        self.n_visible = n_visible # Number of visible nodes\n",
    "        self.lr = lr # Learning rate for the CD algorithm\n",
    "        self.epochs = epochs # Number of iterations to run the algorithm for\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "        self.optimizer = optimizer\n",
    "        self.beta_1=0.9\n",
    "        self.beta_2=0.999\n",
    "        self.epsilon=1e-07\n",
    "        self.m = [0, 0, 0]\n",
    "        self.v = [0, 0, 0]\n",
    "        self.m_batches = {0:[], 1:[], 2:[]}\n",
    "        self.v_batches = {0:[], 1:[], 2:[]}\n",
    "        self.savefile = savefile\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.stagnation = 0\n",
    "        self.previous_loss_before_stagnation = 0\n",
    "        self.progress = []\n",
    "\n",
    "        if torch.cuda.is_available() and gpu==True:  \n",
    "            dev = \"cuda:0\" \n",
    "        else:  \n",
    "            dev = \"cpu\"  \n",
    "        self.device = torch.device(dev)\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        std = 4 * np.sqrt(6. / (self.n_visible + self.n_hidden))\n",
    "        self.W = torch.normal(mean=0, std=std, size=(self.n_hidden, self.n_visible))\n",
    "        self.vb = torch.zeros(size=(1, self.n_visible), dtype=torch.float32)\n",
    "        self.hb = torch.zeros(size=(1, self.n_hidden), dtype=torch.float32)\n",
    "\n",
    "        self.W = self.W.to(self.device)\n",
    "        self.vb = self.vb.to(self.device)\n",
    "        self.hb = self.hb.to(self.device)\n",
    "\n",
    "    def sample_h(self, x):\n",
    "        wx = torch.mm(x, self.W.t())\n",
    "        activation = wx + self.hb\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        if self.mode == 'bernoulli':\n",
    "            return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
    "        else:\n",
    "            return p_h_given_v, torch.add(p_h_given_v, torch.normal(mean=0, std=1, size=p_h_given_v.shape))\n",
    "\n",
    "    def sample_v(self, y):\n",
    "        wy = torch.mm(y, self.W)\n",
    "        activation = wy + self.vb\n",
    "        p_v_given_h =torch.sigmoid(activation)\n",
    "        if self.mode == 'bernoulli':\n",
    "            return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
    "        else:\n",
    "            return p_v_given_h, torch.add(p_v_given_h, torch.normal(mean=0, std=1, size=p_v_given_h.shape))\n",
    "\n",
    "    def adam(self, g, epoch, index):\n",
    "        self.m[index] = self.beta_1 * self.m[index] + (1 - self.beta_1) * g\n",
    "        self.v[index] = self.beta_2 * self.v[index] + (1 - self.beta_2) * torch.pow(g, 2)\n",
    "\n",
    "        m_hat = self.m[index] / (1 - np.power(self.beta_1, epoch)) + (1 - self.beta_1) * g / (1 - np.power(self.beta_1, epoch))\n",
    "        v_hat = self.v[index] / (1 - np.power(self.beta_2, epoch))\n",
    "        return m_hat / (torch.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "    def update(self, v0, vk, ph0, phk, epoch):\n",
    "        dW = (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n",
    "        dvb = torch.sum((v0 - vk), 0)\n",
    "        dhb = torch.sum((ph0 - phk), 0)\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            dW = self.adam(dW, epoch, 0)\n",
    "            dvb = self.adam(dvb, epoch, 1)\n",
    "            dhb = self.adam(dhb, epoch, 2)\n",
    "\n",
    "        self.W += self.lr * dW\n",
    "        self.vb += self.lr * dvb\n",
    "        self.hb += self.lr * dhb\n",
    "\n",
    "    def train(self, dataset):\n",
    "        dataset = dataset.to(self.device)\n",
    "        learning = trange(self.epochs, desc=str('Starting...'))\n",
    "        for epoch in learning:\n",
    "            train_loss = 0\n",
    "            counter = 0\n",
    "            for batch_start_index in range(0, dataset.shape[0]-self.batch_size, self.batch_size):\n",
    "                vk = dataset[batch_start_index:batch_start_index+self.batch_size]\n",
    "                v0 = dataset[batch_start_index:batch_start_index+self.batch_size]\n",
    "                ph0, _ = self.sample_h(v0)\n",
    "\n",
    "                for k in range(self.k):\n",
    "                    _, hk = self.sample_h(vk)\n",
    "                    _, vk = self.sample_v(hk)\n",
    "                phk, _ = self.sample_h(vk)\n",
    "                self.update(v0, vk, ph0, phk, epoch+1)\n",
    "                train_loss += torch.mean(torch.abs(v0-vk))\n",
    "                counter += 1\n",
    "\n",
    "            self.progress.append(train_loss.item()/counter)\n",
    "            details = {'epoch': epoch+1, 'loss': round(train_loss.item()/counter, 4)}\n",
    "            learning.set_description(str(details))\n",
    "            learning.refresh()\n",
    "\n",
    "            if train_loss.item()/counter > self.previous_loss_before_stagnation and epoch>self.early_stopping_patience+1:\n",
    "                self.stagnation += 1\n",
    "                if self.stagnation == self.early_stopping_patience-1:\n",
    "                    learning.close()\n",
    "                    print(\"Not Improving the stopping training loop.\")\n",
    "                    break\n",
    "            else:\n",
    "                self.previous_loss_before_stagnation = train_loss.item()/counter\n",
    "                self.stagnation = 0\n",
    "        learning.close()\n",
    "        if self.savefile is not None:\n",
    "            model = {'W':self.W, 'vb':self.vb, 'hb':self.hb}\n",
    "            torch.save(model, self.savefile)\n",
    "\n",
    "    def load_rbm(self, savefile):\n",
    "        loaded = torch.load(savefile)\n",
    "        self.W = loaded['W']\n",
    "        self.vb = loaded['vb']\n",
    "        self.hb = loaded['hb']\n",
    "\n",
    "        self.W = self.W.to(self.device)\n",
    "        self.vb = self.vb.to(self.device)\n",
    "        self.hb = self.hb.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30539f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 50, 'loss': 0.1656}: 100%|████████████| 50/50 [00:02<00:00, 19.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training: tensor([[-0.2189, -0.1571, -0.1907, -0.0908, -0.1671, -0.2111, -0.2328, -0.2232,\n",
      "         -0.1595, -0.2269]], device='cuda:0')\n",
      "After Loading: tensor([[-0.2189, -0.1571, -0.1907, -0.0908, -0.1671, -0.2111, -0.2328, -0.2232,\n",
      "         -0.1595, -0.2269]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating and running a trial test\n",
    "def trial_dataset():\n",
    "    dataset = []\n",
    "    for _ in range(1000):\n",
    "        t = []\n",
    "        for _ in range(10):\n",
    "            if random.random()>0.75:\n",
    "                t.append(0)\n",
    "            else:\n",
    "                t.append(1)\n",
    "        dataset.append(t)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        t = []\n",
    "        for _ in range(10):\n",
    "            if random.random()>0.75:\n",
    "                t.append(1)\n",
    "            else:\n",
    "                t.append(0)\n",
    "        dataset.append(t)\n",
    "\n",
    "    dataset = np.array(dataset, dtype=np.float32)\n",
    "    np.random.shuffle(dataset)\n",
    "    dataset = torch.from_numpy(dataset)\n",
    "    return dataset\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    dataset = trial_dataset()\n",
    "\n",
    "    rbm = RBM(10, 100, epochs=50, mode='bernoulli', lr=0.001, optimizer='adam', gpu=True, savefile='save_example.pt', early_stopping_patience=50)\n",
    "    print(\"Before Training:\", rbm.vb)\n",
    "    rbm.train(dataset)\n",
    "    print(\"After Training:\", rbm.vb)\n",
    "    rbm.load_rbm('save_example.pt')\n",
    "    print(\"After Loading:\", rbm.vb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d880db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retinalprojkernel",
   "language": "python",
   "name": "retinalprojkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a0bfbe1a51dd6d5a57e057038c49f600f72730721d08b5631c8cdf5ace0d4aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
